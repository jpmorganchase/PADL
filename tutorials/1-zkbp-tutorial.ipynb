{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to ZKBP \n",
    "- A (rust-based) Python module supporting mathematical functionalities needed for Zero-Knowledge Proof (ZKP) described in PADL. \n",
    "\n",
    "The tutorial is split into sections listed below:\n",
    "1) Common Functionality \n",
    "2) Cryptographic Building Block\n",
    "3) Usecase Walkthrough\n",
    "\n",
    "Pre-requisites:\n",
    "1) Python\n",
    "2) ZKPB module listed under the python search path.\n",
    "3) Rust Compilation Tools, rustup (if ZKPB module is not compiled)\n",
    "\n",
    "***\n",
    "\n",
    "## Common Functionality\n",
    "\n",
    "Let's start by importing some basic functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized point: (1, 2)\n",
      "[123, 84, 182, 104, 54, 193, 251, 221, 19, 210, 68, 29, 158, 20, 52, 220, 98, 202, 103, 127, 182, 143, 95, 230, 106, 70, 75, 170, 222, 205, 189, 0, 87, 111, 141, 107, 90, 195, 188, 200, 8, 68, 183, 213, 11, 28, 198, 96, 52, 68, 187, 231, 207, 207, 143, 192, 170, 30, 227, 198, 54, 217, 227, 57]\n",
      "[123, 84, 182, 104, 54, 193, 251, 221, 19, 210, 68, 29, 158, 20, 52, 220, 98, 202, 103, 127, 182, 143, 95, 230, 106, 70, 75, 170, 222, 205, 189]\n",
      "[2, 123, 84, 182, 104, 54, 193, 251, 221, 19, 210, 68, 29, 158, 20, 52, 220, 98, 202, 103, 127, 182, 143, 95, 230, 106, 70, 75, 170, 222, 205, 189]\n",
      "abc 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
      "[23, 22, 9, 158, 53, 95, 48, 125, 221, 49, 101, 3, 196, 0, 247, 17, 53, 63, 49, 198, 8, 134, 137, 240, 185, 177, 242, 242, 145, 152, 137, 74]\n",
      "[23, 22, 9, 158, 53, 95, 48, 125, 221, 49, 101, 3, 196, 0, 247, 17, 53, 63, 49, 198, 8, 134, 137, 240, 185, 177, 242, 242, 145, 152, 137]\n",
      "[2, 23, 22, 9, 158, 53, 95, 48, 125, 221, 49, 101, 3, 196, 0, 247, 17, 53, 63, 49, 198, 8, 134, 137, 240, 185, 177, 242, 242, 145, 152, 137]\n",
      "abc 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
      "[46, 44, 19, 60, 106, 190, 96, 251, 186, 98, 202, 7, 136, 1, 238, 34, 106, 126, 99, 140, 17, 13, 19, 225, 115, 99, 229, 229, 35, 49, 18, 148]\n",
      "[46, 44, 19, 60, 106, 190, 96, 251, 186, 98, 202, 7, 136, 1, 238, 34, 106, 126, 99, 140, 17, 13, 19, 225, 115, 99, 229, 229, 35, 49, 18]\n",
      "[2, 46, 44, 19, 60, 106, 190, 96, 251, 186, 98, 202, 7, 136, 1, 238, 34, 106, 126, 99, 140, 17, 13, 19, 225, 115, 99, 229, 229, 35, 49, 18]\n"
     ]
    }
   ],
   "source": [
    "import zkbp\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "path = os.path.realpath('1-zkbp-tutorial.ipynb')\n",
    "sys.path.append(str(Path(path).parents[1]))  # go up 2 levels [1] to '/pyledger/'\n",
    "from pyledger.zkutils import Commit,r_blend, Token\n",
    "# Some Helper Variable for assertion\n",
    "zero_string = \"000000000000000000000000000000000000000000000000000000000000000000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<sub>Note that multiplicative notation is used instead of additive one for group operation to be consistent with the whitepaper. However, function is expressed usign additive notation because of implementation detail (usage of EC as group).</sub>\n",
    "\n",
    "Generators Generation:\n",
    "- ZKBP make use of two distinct generators $g$ and $h \\in \\mathbb{G}$ which is generated using: `gh = zkbp.gen_GH()`.\n",
    "\n",
    "Key Pair Generation:\n",
    "- `key_pair = zkbp.gen_pb_sk(gh)` generates key in the form of $pk = h^{sk}$.\n",
    "- `pk = key_pair.get_pk()` and `sk = key_pair.get_sk()` to unpack key pair into public key and secret key respectively.\n",
    "- With sk, it is possible to regenrate the corresponding key_pair{pk,sk} using `zkbp.regen_pb_sk(gh,sk)`.\n",
    "\n",
    "Random Scalar Element:\n",
    "- `r=zkbp.gen_r()` generates a random scalar point, $r \\xleftarrow[\\text{}]{\\text{\\$}} \\mathbb{G}$ whereby $\\mathbb{G}$ is the scalar field. \n",
    "- `r=r_blend()` is needed to access native math functions such as `+,-, * `, for example negating can now be done with `rd = -r`.\n",
    "\n",
    "Parsing Scalar Element (such as SK):\n",
    "- `zkbp.to_scalar_from_str(key_pair.get_sk())` parses a string representation of scalar to scalar object.\n",
    "\n",
    "Group Operations, $h^x, g^r, g^{x'}, p^x$ where g,h are generators, p is commit, x' is i32 and x is normal scalar element.\n",
    "- `zkbp.h_to_x(gh, x)` computes $h^x$.\n",
    "- `zkbp.g_to_r(gh, r)` computes $g^r$.\n",
    "- `zkbp.g_to_x(gh, x)` computes $g^x$ whereby x $\\in$ i32.\n",
    "- `zkbp.p_to_x(commit,x)` computes $com^x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123, 84, 182, 104, 54, 193, 251, 221, 19, 210, 68, 29, 158, 20, 52, 220, 98, 202, 103, 127, 182, 143, 95, 230, 106, 70, 75, 170, 222, 205, 189, 0, 87, 111, 141, 107, 90, 195, 188, 200, 8, 68, 183, 213, 11, 28, 198, 96, 52, 68, 187, 231, 207, 207, 143, 192, 170, 30, 227, 198, 54, 217, 227, 57]\n",
      "[123, 84, 182, 104, 54, 193, 251, 221, 19, 210, 68, 29, 158, 20, 52, 220, 98, 202, 103, 127, 182, 143, 95, 230, 106, 70, 75, 170, 222, 205, 189]\n",
      "[2, 123, 84, 182, 104, 54, 193, 251, 221, 19, 210, 68, 29, 158, 20, 52, 220, 98, 202, 103, 127, 182, 143, 95, 230, 106, 70, 75, 170, 222, 205, 189]\n",
      "abc 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
      "[23, 22, 9, 158, 53, 95, 48, 125, 221, 49, 101, 3, 196, 0, 247, 17, 53, 63, 49, 198, 8, 134, 137, 240, 185, 177, 242, 242, 145, 152, 137, 74]\n",
      "[23, 22, 9, 158, 53, 95, 48, 125, 221, 49, 101, 3, 196, 0, 247, 17, 53, 63, 49, 198, 8, 134, 137, 240, 185, 177, 242, 242, 145, 152, 137]\n",
      "[2, 23, 22, 9, 158, 53, 95, 48, 125, 221, 49, 101, 3, 196, 0, 247, 17, 53, 63, 49, 198, 8, 134, 137, 240, 185, 177, 242, 242, 145, 152, 137]\n",
      "abc 21888242871839275222246405745257275088548364400416034343698204186575808495617\n",
      "[46, 44, 19, 60, 106, 190, 96, 251, 186, 98, 202, 7, 136, 1, 238, 34, 106, 126, 99, 140, 17, 13, 19, 225, 115, 99, 229, 229, 35, 49, 18, 148]\n",
      "[46, 44, 19, 60, 106, 190, 96, 251, 186, 98, 202, 7, 136, 1, 238, 34, 106, 126, 99, 140, 17, 13, 19, 225, 115, 99, 229, 229, 35, 49, 18]\n",
      "[2, 46, 44, 19, 60, 106, 190, 96, 251, 186, 98, 202, 7, 136, 1, 238, 34, 106, 126, 99, 140, 17, 13, 19, 225, 115, 99, 229, 229, 35, 49, 18]\n",
      "serialized point: [215, 145, 7, 164, 73, 34, 145, 62, 117, 233, 224, 188, 73, 213, 187, 196, 206, 105, 72, 4, 203, 45, 125, 133, 49, 5, 254, 175, 140, 1, 5, 139]\n",
      "serialized point: [215, 145, 7, 164, 73, 34, 145, 62, 117, 233, 224, 188, 73, 213, 187, 196, 206, 105, 72, 4, 203, 45, 125, 133, 49, 5, 254, 175, 140, 1, 5, 139]\n",
      "[0]\n",
      "[123]\n",
      "[13]\n"
     ]
    }
   ],
   "source": [
    "# generate g and h\n",
    "gh = zkbp.gen_GH()\n",
    "\n",
    "# generate public and secret key\n",
    "key_pair = zkbp.gen_pb_sk(gh)\n",
    "pk = key_pair.get_pk()\n",
    "sk = key_pair.get_sk()\n",
    "sk_scalar_obj  = zkbp.to_scalar_from_str(key_pair.get_sk())\n",
    "recons_key_pair = zkbp.regen_pb_sk(gh,sk_scalar_obj)\n",
    "assert recons_key_pair.get_pk() == pk and recons_key_pair.get_sk()==sk, \"Reconstructed PK or SK does not agree\"\n",
    "\n",
    "# generate a random scalar r\n",
    "r = zkbp.gen_r()\n",
    "# print(f'r is {r.get}')\n",
    "\n",
    "# This r does not have add,mult,neg functions, so we use class r_blend()\n",
    "r = r_blend()\n",
    "rd = -r \n",
    "#print(f'r_blend is {r.to_str()} and rd+r is {(rd+r).to_str()}')\n",
    "#print(f'minus r is {rd.to_str()} and length is {len(rd.to_str())}')\n",
    "\n",
    "\n",
    "# Group Operations\n",
    "hx = zkbp.h_to_x(gh, r.val)\n",
    "gr = zkbp.g_to_r(gh, r.val)\n",
    "gx = zkbp.g_to_x(gh, 123)\n",
    "\n",
    "value = 13\n",
    "r = zkbp.gen_r()\n",
    "cm = zkbp.commit(value, r, gh)\n",
    "comx = zkbp.p_to_x(cm, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Cryptographic Building Block\n",
    "\n",
    "Pedersen commitment,  $c = g^vh^r$\n",
    "- `r = zkbp.gen_r()` and `value = 13` generates a value 13 with blinding factor $r$.\n",
    "- `cm = zkbp.commit(value,r, gh)` to commit value 13, where $g^{13}h^{r}$.\n",
    "- Alternatively, `r = r_blend()` and `cmd = Commit(gh,value,r)` also generate a commitment.\n",
    "- `zkbp.add(cm,cm)` can be used to add two pedersen commitments, while `zkbp.sub(cm,cm)` subtract one from the other.\n",
    "- ZKBP also provides the capability to add two stringified commit together via `zkbp.add_value_commits(commit_l,commit_r)` and subtraction via `zkbp.sub_value_commits(commit_l,commit_r)`.\n",
    "- CommitmentPoint Deserialization from String can be done using `zkbp.from_str(comm_str_repr)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13]\n",
      "[26]\n",
      "serialized point: [62, 249, 20, 85, 145, 77, 126, 44, 84, 145, 132, 147, 107, 12, 239, 22, 0, 69, 91, 7, 222, 35, 150, 87, 123, 62, 235, 106, 216, 77, 213, 160]\n",
      "serialized point: [62, 249, 20, 85, 145, 77, 126, 44, 84, 145, 132, 147, 107, 12, 239, 22, 0, 69, 91, 7, 222, 35, 150, 87, 123, 62, 235, 106, 216, 77, 213, 160]\n",
      "serialized point: [186, 215, 175, 134, 72, 105, 247, 106, 17, 50, 73, 63, 129, 231, 123, 87, 232, 47, 173, 54, 138, 62, 195, 69, 134, 96, 225, 153, 187, 224, 43, 174]\n",
      "serialized point: [186, 215, 175, 134, 72, 105, 247, 106, 17, 50, 73, 63, 129, 231, 123, 87, 232, 47, 173, 54, 138, 62, 195, 69, 134, 96, 225, 153, 187, 224, 43, 174]\n",
      "serialized point: [62, 249, 20, 85, 145, 77, 126, 44, 84, 145, 132, 147, 107, 12, 239, 22, 0, 69, 91, 7, 222, 35, 150, 87, 123, 62, 235, 106, 216, 77, 213, 160]\n",
      "stringvalues 0000000000000000000000000000000000000000000000000000000000000040\n",
      "serialized point: [62, 249, 20, 85, 145, 77, 126, 44, 84, 145, 132, 147, 107, 12, 239, 22, 0, 69, 91, 7, 222, 35, 150, 87, 123, 62, 235, 106, 216, 77, 213, 160]\n",
      "serialized point: [186, 215, 175, 134, 72, 105, 247, 106, 17, 50, 73, 63, 129, 231, 123, 87, 232, 47, 173, 54, 138, 62, 195, 69, 134, 96, 225, 153, 187, 224, 43, 174]\n",
      "serialized point: [186, 215, 175, 134, 72, 105, 247, 106, 17, 50, 73, 63, 129, 231, 123, 87, 232, 47, 173, 54, 138, 62, 195, 69, 134, 96, 225, 153, 187, 224, 43, 174]\n",
      "serialized point: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64]\n",
      "serialized point: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64]\n",
      "serialized point: [186, 215, 175, 134, 72, 105, 247, 106, 17, 50, 73, 63, 129, 231, 123, 87, 232, 47, 173, 54, 138, 62, 195, 69, 134, 96, 225, 153, 187, 224, 43, 174]\n",
      "serialized point: [186, 215, 175, 134, 72, 105, 247, 106, 17, 50, 73, 63, 129, 231, 123, 87, 232, 47, 173, 54, 138, 62, 195, 69, 134, 96, 225, 153, 187, 224, 43, 174]\n",
      "serialized point: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64]\n",
      "serialized point: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64]\n",
      "serialized point: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64]\n"
     ]
    }
   ],
   "source": [
    "# Pedersen commitment generation\n",
    "import zkbp.zkbp\n",
    "\n",
    "value = 13\n",
    "r = zkbp.gen_r()\n",
    "cm = zkbp.commit(value, r, gh)\n",
    "\n",
    "# Commitment operations (Add, Sub)\n",
    "cm_sample = zkbp.commit(value+value, r.sum(r), gh)\n",
    "new_comm = zkbp.add(cm,cm)\n",
    "assert new_comm.get == cm_sample.get, \"commit3 = Add(commit1,commit2) should satisfies commit3 === Commit(commit1.value + commit2.value, commit1.r + commit2.r)\"\n",
    "\n",
    "new_comm = zkbp.sub(cm,cm)\n",
    "assert new_comm.is_zero(), \"Sub(commit1,commit2) should return 0 if commit1 === commit2\"\n",
    "\n",
    "# Commitment operations for commitment string, returning the same string structure\n",
    "new_comm_str_repr = zkbp.add_value_commits(cm.get,cm.get)\n",
    "assert new_comm_str_repr == cm_sample.get, \"commit3 = Add(commit1,commit2) should satisfies commit3 === Commit(commit1.value + commit2.value, commit1.r + commit2.r)\"\n",
    "\n",
    "new_comm_str_repr = zkbp.sub_value_commits(cm.get,cm.get)\n",
    "\n",
    "print('stringvalues',zkbp.from_str(new_comm_str_repr).get)\n",
    "\n",
    "assert zkbp.from_str(new_comm_str_repr).is_zero(), \"Sub(commit1,commit2) should return 0 if commit1 === commit2\"\n",
    "\n",
    "# Construct a commit point from its string representation\n",
    "# new_cm := sub(cm,cm) should be that new_cm === from_str(new_cm_str_repr)\n",
    "new_comm = zkbp.sub(cm,cm)\n",
    "new_comm_str_repr = zkbp.sub_value_commits(cm.get,cm.get)\n",
    "new_cm = zkbp.from_str(new_comm_str_repr)\n",
    "assert new_comm.get == new_cm.get, \"Converting from commitment string representation should remain the same.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Token, $tk = h^{sk \\cdot r}$\n",
    "- `tk = key_pair.to_token(r.val)` generate token of the form $tk = h^{sk \\cdot r}$.\n",
    "- `tk = to_token_from_str(tkn_str_repr)` generate token from token string representation.\n",
    "- `zkbp.print_token(tk)` to print token. Note that the stdout used by RUST might not be redirected correctly in jupyternotebook.\n",
    "- `zkbp.add_token(tk1,tk2)` can be used to add two token, while `zkbp.sub_token(tk1,tk2)` subtract one from the other.\n",
    "- `zkbp.to_token_from_pk(pk, scalar_r)` can also be used to generate token.\n",
    "\n",
    "Commit - Token Operations\n",
    "- ZKBP also provides the capability to add commit and token together via `zkbp.add_commit_token(commit,token)` and subtraction via `zkbp.sub_commit_token(commit,token)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c8826e03e32e1d9a246110fdc1e33a353e298b6bbca7cc788e158d7e13bed601[13]\n",
      "serialized point: [200, 130, 110, 3, 227, 46, 29, 154, 36, 97, 16, 253, 193, 227, 58, 53, 62, 41, 139, 107, 188, 167, 204, 120, 142, 21, 141, 126, 19, 190, 214, 1]\n",
      "\n",
      "82fede71665e5e34a77207ea0024cdeabe4b98aaddd539cb15f3e987221a332f\n",
      "serialized point: [130, 254, 222, 113, 102, 94, 94, 52, 167, 114, 7, 234, 0, 36, 205, 234, 190, 75, 152, 170, 221, 213, 57, 203, 21, 243, 233, 135, 34, 26, 51, 47]\n",
      "ok\n",
      "serialized point: [130, 254, 222, 113, 102, 94, 94, 52, 167, 114, 7, 234, 0, 36, 205, 234, 190, 75, 152, 170, 221, 213, 57, 203, 21, 243, 233, 135, 34, 26, 51, 47]\n",
      "serialized point: [130, 254, 222, 113, 102, 94, 94, 52, 167, 114, 7, 234, 0, 36, 205, 234, 190, 75, 152, 170, 221, 213, 57, 203, 21, 243, 233, 135, 34, 26, 51, 47]\n",
      "serialized point: [130, 254, 222, 113, 102, 94, 94, 52, 167, 114, 7, 234, 0, 36, 205, 234, 190, 75, 152, 170, 221, 213, 57, 203, 21, 243, 233, 135, 34, 26, 51, 47]\n",
      "Bn254Point {\n",
      "    purpose: \"scalar_mul\",\n",
      "    ge: (21348993460073821730601366906563138730899295340235912757610158606668626067074, 9676957882863379721242065835348392766956523665037559424660155327204209418758),\n",
      "}\n",
      "serialized point: [70, 121, 32, 126, 62, 128, 245, 245, 190, 200, 62, 202, 6, 117, 8, 109, 186, 158, 107, 171, 224, 203, 144, 41, 158, 204, 173, 172, 134, 103, 60, 21]\n",
      "serialized point: [70, 121, 32, 126, 62, 128, 245, 245, 190, 200, 62, 202, 6, 117, 8, 109, 186, 158, 107, 171, 224, 203, 144, 41, 158, 204, 173, 172, 134, 103, 60, 21]\n",
      "[0]\n",
      "serialized point: [212, 171, 170, 101, 218, 185, 196, 152, 58, 21, 39, 177, 45, 108, 210, 124, 209, 143, 212, 131, 115, 50, 244, 249, 123, 28, 40, 14, 56, 21, 37, 167]\n",
      "serialized point: [212, 171, 170, 101, 218, 185, 196, 152, 58, 21, 39, 177, 45, 108, 210, 124, 209, 143, 212, 131, 115, 50, 244, 249, 123, 28, 40, 14, 56, 21, 37, 167]\n",
      "serialized point: [215, 145, 7, 164, 73, 34, 145, 62, 117, 233, 224, 188, 73, 213, 187, 196, 206, 105, 72, 4, 203, 45, 125, 133, 49, 5, 254, 175, 140, 1, 5, 139]\n",
      "serialized point: [51, 171, 6, 116, 114, 110, 34, 58, 80, 128, 1, 102, 41, 187, 38, 248, 92, 185, 13, 107, 11, 224, 21, 169, 123, 74, 241, 142, 16, 73, 162, 134]\n",
      "serialized point: [51, 171, 6, 116, 114, 110, 34, 58, 80, 128, 1, 102, 41, 187, 38, 248, 92, 185, 13, 107, 11, 224, 21, 169, 123, 74, 241, 142, 16, 73, 162, 134]\n",
      "[13]\n",
      "serialized point: [181, 106, 175, 13, 60, 108, 72, 86, 225, 252, 241, 39, 230, 102, 248, 202, 82, 165, 95, 165, 115, 222, 148, 118, 78, 2, 8, 157, 98, 202, 160, 2]\n",
      "serialized point: [181, 106, 175, 13, 60, 108, 72, 86, 225, 252, 241, 39, 230, 102, 248, 202, 82, 165, 95, 165, 115, 222, 148, 118, 78, 2, 8, 157, 98, 202, 160, 2]\n"
     ]
    }
   ],
   "source": [
    "# Alternative option (Note that this is different object, Commit is python Commit-helper_class while zkbp.commit return a rust commit)\n",
    "r = r_blend()\n",
    "cmd = Commit(gh, value, r)\n",
    "print(cmd.eval.get)\n",
    "\n",
    "# we can also create a token that is just the public key raised to r\n",
    "r = r_blend()\n",
    "tk = key_pair.to_token(r.val)\n",
    "print(tk.get)\n",
    "\n",
    "# Token reconstruction\n",
    "tk_recons = zkbp.to_token_from_str(tk.get)\n",
    "assert tk.get == tk_recons.get, \"Reconstructed token should not differ in value.\"\n",
    "\n",
    "# Printing: Might not print out token in the notebook. (Will only print out ok)\n",
    "print(zkbp.print_token(tk))\n",
    "\n",
    "# Adding and Subtraction\n",
    "r = r_blend()\n",
    "tk1 = key_pair.to_token(r.val)\n",
    "r2 = r_blend()\n",
    "tk2 = key_pair.to_token(r2.val)\n",
    "tk_ref = key_pair.to_token(r.val.sum(r2.val))\n",
    "tk_sum = zkbp.add_token(tk1,tk2)\n",
    "assert tk_sum.get == tk_ref.get, \"Adding token should be the same as adding the blinding factor before constructing token\"\n",
    "tk_ref = key_pair.to_token(r.val.sum(r2.val.neg()))\n",
    "tk_sum = zkbp.sub_token(tk1,tk2)\n",
    "assert tk_sum.get == tk_ref.get, \"Subtracting token should be the same as subtracting the blinding factor before constructing token\"\n",
    "\n",
    "# PK to token\n",
    "ref_token = key_pair.to_token(r.val)\n",
    "token = zkbp.to_token_from_pk(key_pair.get_pk(), r.val)\n",
    "assert token.get == ref_token.get, \"Tokens should be equal.\"\n",
    "\n",
    "#Commit - token combined operation\n",
    "value = 13\n",
    "r = zkbp.gen_r()\n",
    "cm = zkbp.commit(value, r, gh)\n",
    "r = r_blend()\n",
    "tk1 = key_pair.to_token(r.val)\n",
    "element1 = zkbp.sub_commit_token(cm, tk1)\n",
    "element2 = zkbp.add_commit_token(element1, tk1)\n",
    "assert cm.get == element2.get, \"Subtracting and then adding back the same element should be behave like an identity function.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## PoK ZKP Showcase\n",
    "\n",
    "### ZKProof of Knowledge of dlog\n",
    "- Let's start with some arbitrary value `sval = 10`, sample randomness `r = r_blend()`.\n",
    "- Commit to the chosen value with `cm = Commit(gh,sval,r)` and generate a token with `tk = key_pair.to_token(r.val)`.\n",
    "- Let's assume we want to prove the knowledge of $sk$ in the token $h^{sk \\cdot r}$. \n",
    "- Start by calculating a new generator base with $\\frac{cm}{g^{sval}} = g^{sval-sval}h^{r} = h^r$.\n",
    "- Perform operation $\\frac{cm}{g^{sval}}$ using  `h2r = zkbp.sub(zkbp.from_str(cm.eval.get), zkbp.from_str(zkbp.g_to_x(gh, sval).get))`.\n",
    "- Now we can generate proof of knowledge of $sk$ in $h^{r \\cdot sk}$ with $h^r$ as the generator.\n",
    "- Generate proof using `pr = zkbp.sigma_dlog_proof_explicit_sha256(tk,key_pair,h2r)`.\n",
    "- Verify proof using `t = zkbp.sigma_dlog_proof_explicit_verify_sha256(pr,h2r,tk)`.\n",
    "- Note that variant without sha256 suffix uses sha512.\n",
    "<!-- Token, tk is not used in sigma_dlog_proof_explicit_sha256, zkbp.sigma_dlog_proof_explicit_sha256(.. ,key_pair,h2r) where key_pair.sk is the witness and h2r is the generator-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n",
      "serialized point: [82, 115, 170, 180, 228, 167, 76, 60, 149, 143, 156, 127, 189, 217, 54, 103, 90, 85, 6, 190, 208, 219, 48, 147, 34, 235, 19, 3, 196, 165, 182, 168]\n",
      "[10]\n",
      "serialized point: [36, 177, 38, 100, 133, 236, 151, 217, 196, 68, 244, 124, 171, 18, 180, 157, 175, 215, 12, 80, 96, 19, 94, 189, 134, 235, 214, 62, 218, 180, 104, 33]\n",
      "serialized point: [96, 61, 237, 7, 166, 45, 227, 139, 166, 230, 172, 76, 72, 169, 152, 200, 242, 214, 80, 253, 235, 50, 37, 172, 198, 96, 13, 253, 255, 236, 101, 155]\n",
      "serialized point: [112, 190, 204, 164, 174, 26, 158, 174, 134, 114, 192, 4, 132, 144, 143, 89, 96, 42, 113, 87, 205, 147, 115, 1, 97, 9, 70, 245, 122, 56, 122, 171]\n",
      "Proof of Knowledge for DLog:  True\n"
     ]
    }
   ],
   "source": [
    "# proof of knowledge: lets create a zk proof and then verify it\n",
    "\n",
    "#g2val = zkbp.g_to_x(gh,value)\n",
    "sval = 10\n",
    "r = r_blend()\n",
    "cm = Commit(gh,sval,r)\n",
    "tk = key_pair.to_token(r.val)\n",
    "#h2r = Commit(gh,0,r).eval\n",
    "#tk = key_pair.to_token(r.val)\n",
    "h2r = zkbp.sub(zkbp.from_str(cm.eval.get), zkbp.from_str(zkbp.g_to_x(gh, sval).get))\n",
    "pr = zkbp.sigma_dlog_proof_explicit_sha256(tk,key_pair,h2r)\n",
    "t = zkbp.sigma_dlog_proof_explicit_verify_sha256(pr,h2r,tk)\n",
    "\n",
    "# print(pr)\n",
    "print(\"Proof of Knowledge for DLog: \", t)\n",
    "assert t, \"Proof not verified.\"\n",
    "#Â hash order: pk_t_rand_commitment, h2r, pk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZKProof of Knowledge of DLOG equivalent\n",
    "- Let's start with some arbitrary value `sval = 10`, sample randomness `r = r_blend()`.\n",
    "- Commit to the chosen value with `cm = Commit(gh,sval,r)` and generate a token with `tk = key_pair.to_token(r.val)`.\n",
    "- Let's assume we want to prove the equivalent of sk in token_sum and pk\n",
    "- Note that `sigma_eq_dlog_proof_sha256` requires token_sum/sk, therefore we also supply h_sum_r (that is token without sk) at the end.\n",
    "- In short, we do `zkbp.sigma_eq_dlog_proof_sha256(tk, key_pair, gh, h_sum_r)` to compute the proof.\n",
    "- `zkbp.sigma_eq_dlog_verify_sha256(pr, gh, h_sum_r, tk)` to verify the proof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = r_blend()\n",
    "val = 10\n",
    "tk = key_pair.to_token(r.val)\n",
    "cm = Commit(gh,val,r).eval\n",
    "\n",
    "h_sum_r = zkbp.sub(cm, zkbp.g_to_x(gh, val)) # g^vh^r / g^v = h^r\n",
    "pr = zkbp.sigma_eq_dlog_proof_sha256(tk, key_pair, gh, h_sum_r) #tk = h^{sk . r}, h_sum_r = h^r\n",
    "# cm = Commit(gh,val,r).eval\n",
    "# print(pr)\n",
    "res = zkbp.sigma_eq_dlog_verify_sha256(pr, gh, h_sum_r, tk)\n",
    "print(\"Proof of DLOG-Equiv: \",res)\n",
    "assert res, \"Proof of DLOG-Equiv Failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZKProof of Knowledge of Consistency\n",
    "\n",
    "- Proof of consistency assert that for a given commitment and token, it is of the form of $\\exists v,r : com = g^vh^r \\wedge tk = pk^r$.\n",
    "- `p= zkbp.consistency_proof(v,r,gh=(g,h), cm, tk, pk)` computes the proof.\n",
    "- `zkbp.consistency_proof_verify(p, gh=(g,h), cm, tk, pk)` verifies the proof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = r_blend()\n",
    "rd = -r \n",
    "newr = rd + r\n",
    "#print(newr.to_str())\n",
    "v = 0\n",
    "tk = key_pair.to_token(r.val)\n",
    "# print(f'tk is {tk.get}')\n",
    "cm = Commit(gh,v,r).eval\n",
    "# print(f'cm is {cm.get}')\n",
    "# print(f'pk is {pk}')\n",
    "pr = zkbp.consistency_proof(v,r.val,gh,cm,tk,pk)\n",
    "# print(pr)\n",
    "result = zkbp.consistency_proof_verify(proof=pr, gh=gh, ped_cm=cm, token=tk,\n",
    "                                               pubkey=key_pair.get_pk())\n",
    "print(\"Proof of Consistency:\", result)\n",
    "assert(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Range Proof \n",
    "\n",
    "-  `zkbp.range_proof_single(n_bit, val=v, gh=gh, r=r.val)` proves that 0 < v < $2^n$.\n",
    "- `zkbp.range_proof_single_verify(pr, n_bit, gh, cm.eval)` verifies the proof $:g^v h^r \\wedge 0 < v < 2^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a range proof\n",
    "v = 255 #(try -1, 1 )\n",
    "r = r_blend()\n",
    "gh = zkbp.gen_GH()\n",
    "cm = Commit(gh, v, r)\n",
    "n_bit = 8 #(try 1, 2)\n",
    "pr = zkbp.range_proof_single(n_bit=n_bit, val=v, gh=gh, r=r.val)\n",
    "res = zkbp.range_proof_single_verify(pr, n_bit, gh, cm.eval)\n",
    "# print(pr)\n",
    "print(res)\n",
    "assert res, \"Range proof not verified.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLog Calculation\n",
    "\n",
    "- Given a token, $tk = pk^r$ and commitment, $cm = g^vh^r$, computes $v$.\n",
    "- `zkbp.get_brut_v(cm,tk,gh,key_pair,max_range)` computes the aforementioned v by bruteforcing from $g^{v\\cdot sk} = \\frac{cm^{sk}}{tk}$.\n",
    "<!-- 2^16 (65536) takes 8s on Intel Year-2020 Laptop-CPU  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 100\n",
    "r = zkbp.gen_r()\n",
    "cm = zkbp.commit(v, r, gh)\n",
    "tk = key_pair.to_token(r)\n",
    "max_range = 1000000\n",
    "\n",
    "computed_v = zkbp.get_brut_v(cm,tk,gh,key_pair,max_range)\n",
    "assert v==computed_v, \"Brute-forcing g^v should give the correct v.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Commit with SHA256\n",
    "\n",
    "- hash_test1(cm1), hash_test2(cm1,cm2), hash_test3(cm1,cm2,cm3) computes hashes with input of {1,2,3} commits respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 10\n",
    "r = zkbp.gen_r()\n",
    "cm = zkbp.commit(v, r, gh)\n",
    "\n",
    "hash1 = zkbp.hash_test1(cm)\n",
    "hash2 = zkbp.hash_test2(cm,cm)\n",
    "hash3 = zkbp.hash_test3(cm,cm,cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # # Helper Function\n",
    "# for attr in dir(r):\n",
    "#     print(\"obj.%s = %r\" % (attr, getattr(r, attr)))\\ -->\n",
    "\n",
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
